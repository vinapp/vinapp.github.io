<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://vinapp.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://vinapp.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-06-02T09:24:26+00:00</updated><id>https://vinapp.github.io/feed.xml</id><title type="html">Vinod Appajanna</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Voice Based Custom Search</title><link href="https://vinapp.github.io/blog/2023/voice-based-customsearch/" rel="alternate" type="text/html" title="Voice Based Custom Search" /><published>2023-06-01T00:00:00+00:00</published><updated>2023-06-01T00:00:00+00:00</updated><id>https://vinapp.github.io/blog/2023/voice-based-customsearch</id><content type="html" xml:base="https://vinapp.github.io/blog/2023/voice-based-customsearch/"><![CDATA[<h1 id="abstract">Abstract</h1>
<p>A Hybrid Ionic mobile app (<a href="https://github.com/vinapp/voice-based-customsearch">voice-based-customsearch</a>) that accepts Indic voice-based commands to search the Indic (domain) scoped data. The Indic domain data includes data such as music/tourism/theertha-stala/spiritual/yoga/etc. Here the voice to text conversion happens via JS WebSpeech API’s. Also, the search engine includes custom Google search engine, Wikipedia search engine or similar one (one or more of these). The Google custom Search helps in creating our own search engine for a choosen topic collected from a group of websites of our choosing.</p>

<h2 id="motivation">Motivation</h2>
<ul>
  <li>Explore the client and service-side aspects of voice-to-text &amp; tts.</li>
  <li>Investigate existing publicly available search engines or APIs for querying domain-specific public data, rather than building our own.</li>
  <li>Extend the search to include morphological analysis and generation for Indian languages to improve the search engine results. The output of the search engine depends on the input words. If a word is provided in an inflected form and is not present in the search engine’s lexicon, it will affect the search engine’s output.</li>
  <li>One of the most common use case that people want is to be able to search for information by speaking a query into a device.</li>
  <li>More/Extend (Own Pretrained ASR model + TTS + Q/A for websites)</li>
</ul>

<h2 id="high-level-flow">High Level Flow</h2>
<p><img width="828" alt="image" src="https://github.com/vinapp/voice-based-customsearch/assets/8567548/469a817a-00da-4691-b346-0422286b968d" /></p>

<h2 id="asrtts--webspeech-api--backend-flow">ASR/TTS – WebSpeech API – Backend flow</h2>
<ul>
  <li>Web Speech API is a web technology specification that is developed and maintained by the World Wide Web Consortium (W3C)</li>
  <li>Web Speech API is a JavaScript API that allows developers to incorporate speech recognition and synthesis capabilities into web - applications.</li>
  <li>The implementation of the Web Speech API can vary slightly between different browsers</li>
  <li>Each browser implements the Web Speech API using its own underlying speech recognition and synthesis engines</li>
  <li>Google Chrome’s speech recognition engine requires an active internet connection to work, as it relies on cloud-based speech recognition technology that processes the audio input on remote servers.</li>
  <li>Speech recognition engines, such as the one used in Windows 10, can work offline to some extent by using pre-installed language models and recognition algorithm</li>
</ul>

<h2 id="apis-search--web-speech">API’s (Search &amp; Web Speech)</h2>
<ul>
  <li>Web Search API’s
    <ul>
      <li>https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition</li>
      <li>https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis</li>
    </ul>
  </li>
  <li>Tensor flow JS:
    <ul>
      <li>https://www.tensorflow.org/lite</li>
      <li>https://github.com/tensorflow/tfjs</li>
      <li>https://github.com/tulasiram58827/TTS_TFLite/tree/main/models</li>
    </ul>
  </li>
  <li>Google Custom Search
    <ul>
      <li>https://developers.google.com/custom-search/v1/overview</li>
    </ul>
  </li>
  <li>Wiki Search
    <ul>
      <li>https://www.mediawiki.org/wiki/API:Search</li>
      <li>https://kn.wikipedia.org/w/api.php?action=query&amp;format=json&amp;list=search&amp;srsearch=ಶ್ರೀರಂಗ&amp;sroffset=10</li>
    </ul>
  </li>
  <li>Wiki Geo Search
    <ul>
      <li>https://www.mediawiki.org/wiki/API:Search</li>
      <li>https://kn.wikipedia.org/w/api.php?action=query&amp;format=json&amp;list=search&amp;srsearch=ಶ್ರೀರಂಗ&amp;sroffset=10</li>
      <li>https://en.wikipedia.org/w/api.php?action=query&amp;prop=coordinates&amp;format=json&amp;titles=Tirupati</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>https://en.wikipedia.org/w/api.php?action=query&amp;list=geosearch&amp;format=json&amp;gscoord=10.87</td>
              <td>78.68&amp;gsradius=10000&amp;gslimit=5</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>Open Search API’s
    <ul>
      <li>https://www.mediawiki.org/wiki/API:Search</li>
    </ul>
  </li>
</ul>

<h2 id="search-engine---configuration">Search Engine - Configuration</h2>
<p><img width="910" alt="image" src="https://github.com/vinapp/voice-based-customsearch/assets/8567548/693e4bfd-c40d-476d-aacb-c4b1afcc4135" /></p>

<h2 id="search-options">Search Options</h2>
<p><img width="809" alt="image" src="https://github.com/vinapp/voice-based-customsearch/assets/8567548/224853cf-6e16-40ed-b695-5c85de4eb7a1" /></p>

<h2 id="reference-links">Reference links</h2>
<p>### Custom Google Search</p>
<ul>
  <li>https://developers.google.com/custom-search/v1/overview</li>
  <li>https://programmablesearchengine.google.com/controlpanel/all
  ### Maps</li>
  <li>https://openlayers.org/</li>
  <li>https://openlayers.org/en/latest/examples/</li>
  <li>https://wiki.openstreetmap.org/wiki/Main_Page</li>
</ul>

<h1 id="source-code">Source Code</h1>
<ul>
  <li>https://github.com/vinapp/voice-based-customsearch</li>
</ul>]]></content><author><name></name></author><category term="ai" /><category term="ml" /><summary type="html"><![CDATA[Voice Based Custom Search]]></summary></entry><entry><title type="html">A Study &amp;amp; POC - Transpiler for Indian Languages using TypesSripts</title><link href="https://vinapp.github.io/blog/2022/study-typescript-based-transpiler-for-indian-languages/" rel="alternate" type="text/html" title="A Study &amp;amp; POC - Transpiler for Indian Languages using TypesSripts" /><published>2022-10-31T00:00:00+00:00</published><updated>2022-10-31T00:00:00+00:00</updated><id>https://vinapp.github.io/blog/2022/study-typescript-based-transpiler-for-indian-languages</id><content type="html" xml:base="https://vinapp.github.io/blog/2022/study-typescript-based-transpiler-for-indian-languages/"><![CDATA[<h1 id="abstractidea">Abstract/Idea</h1>
<p><a href="https://github.com/vinapp/indic-transpiler-demo">Here</a> have tried to explore and demonstrate on how we can use TypeScripts for transpiling custom keywords chosen from our own lipi’s/Scripts (indic - Hindi/Kan/Telugu etc..) into JS English version. In this way we can extend the Traspiler for any scripts.</p>

<h2 id="why-typescript-">Why TypeScript ?</h2>
<ul>
  <li>Strong static typing</li>
  <li>Object oriented programming</li>
  <li>Better code maintenance</li>
  <li>Confidence to the developer</li>
  <li>Improved productivity</li>
  <li>Large Plain Javascripts are difficult to handle</li>
</ul>

<h2 id="flow-diagram">Flow diagram</h2>
<p><img width="645" alt="image" src="https://github.com/vinapp/vinapp.github.io/assets/8567548/dd94ebc1-83da-413b-ab1a-03488cfd0e16" /></p>

<h2 id="technologylanguagesmodulestools">Technology/Languages/modules/tools</h2>
<ul>
  <li>Node</li>
  <li>TypeScript</li>
  <li>Jest (JavaScript Testing Framework)</li>
  <li>Vite (bootstrap-frontend)</li>
</ul>

<h2 id="references">References</h2>
<ul>
  <li>https://vedic-lang.github.io/</li>
  <li>https://omlang.com/</li>
  <li>https://github.com/microsoft/TypeScript</li>
  <li>https://github.com/basarat/demo-compiler</li>
  <li>https://astexplorer.net/</li>
  <li>https://github.com/jamiebuilds/the-super-tiny-compiler/blob/master/the-super-tiny-compiler.js?s=09</li>
</ul>

<h1 id="source-code">Source Code</h1>
<ul>
  <li>https://github.com/vinapp/indic-transpiler-demo</li>
  <li>https://github.com/Umesh-k26/illustrate-indic-js-transpiler - Using JiSON Parser</li>
</ul>

<p><img width="945" alt="indic-transpiler" src="https://github.com/vinapp/vinapp.github.io/assets/8567548/3c77b1c0-d3ed-4571-ba51-b482f62942c5" /></p>]]></content><author><name></name></author><category term="ai" /><category term="ml" /><summary type="html"><![CDATA[A Study & POC - Transpiler for Indian Languages using TypesSripts]]></summary></entry><entry><title type="html">Multi class image generation using Stable Diffusion</title><link href="https://vinapp.github.io/blog/2022/multi-class-image-generation-stablediffusion/" rel="alternate" type="text/html" title="Multi class image generation using Stable Diffusion" /><published>2022-10-01T10:00:00+00:00</published><updated>2022-10-01T10:00:00+00:00</updated><id>https://vinapp.github.io/blog/2022/multi-class-image-generation-stablediffusion</id><content type="html" xml:base="https://vinapp.github.io/blog/2022/multi-class-image-generation-stablediffusion/"><![CDATA[<h2 id="voice-based-multi-class-image-generation-using-stable-diffusion">Voice based Multi class image generation using Stable Diffusion</h2>

<h3 id="problem-statement">Problem Statement</h3>

<p>We have seen the generative models like Diffusion Model for some time. With the release of Stable Diffusion’s (released recently as early as August 2022) the code and the model weights have been released publicly allowing users to have more control over the topic picture and then controlling the diffusion model using text-based inputs, we can do a transfer learning for a subject &amp; an identifier with just few images. Also, we see that there are only few articles on these and especially only covering a single class transfer learning. So, taking advantage of this we see an option of training (our own set of images) with different classes (with our own identifiers) and clubbing multiple classes. So with this we can issue a single sentence/text which covers all the classes/identifiers and generate images based our custom image set. Also, extending text-to-image to voice-to-text-to-image helps.</p>

<h3 id="task">Task</h3>

<ul>
  <li>For our own set of images train with the pre-trained model with multiple classes/subjects and the identifier’s</li>
  <li>Add a support to speech to text option on top of text to image option</li>
  <li>Find the group of classes/subject that we want to train and test the same with different text’s (sentences). Testing could be manual or based on the labelled data.</li>
</ul>

<h3 id="why-is-it-interesting-">Why is it interesting ?</h3>

<p>Interesting because imagination becomes real in the image. One of the powerful use case would be in slide preparation or storyboard creation wherein we can generate images on our own set of images (grouped into different class) that are trained via transfer learning. This would save time and also have multiple imaginative image generation options. Also, here we have some control over the image generation since its operating on our own set of images.</p>

<p>Also, although there are articles/blogs on stable diffusion (past 2 months) on training individual class (CAT/DOG et) but we haven’t seen any articles (so far) on how we can use set/group of images and how we can iteratively create multiple classes one of top of each other along with publicly available dream booth diffusion model data. So, this could be a nice candidate for publishing a blog that would be very useful to people.</p>

<p>Eg:</p>

<p><strong>photo of &lt;custom-identifier&gt; &lt;class-name&gt;</strong></p>

<ul>
  <li>photo of &lt;xyz&gt; &lt;temple&gt; —&gt; Single class</li>
  <li>photo of &lt;kar&gt; &lt;forest&gt; —&gt; Single class</li>
  <li>photo of &lt;xyz&gt; &lt;temple&gt; in &lt;kar&gt; &lt;forest&gt; and sunlight —&gt; Mixing multiple custom classes</li>
</ul>

<p>This one especially touches all/most of ML aspects like deep learning, un-supervised, synthesized data, labelled data, text summarization, voice to text, text to image etc. This one will serve as a nice learning experience for all of us.</p>

<p>Example Usecase: Think of having a plugin (to powerpoint/google slides) that can integrate custom generative model to generate our own choice of images and this would be very powerful… For eg: Every week if we need to publish some articles (say related to temples) and I need some creative images and it’s very difficult to get the images of our own choice and that too for say 100’s of articles. So in that case this is very useful and we don’t have bother out <a href="https://huggingface.co/CompVis/stable-diffusion">licensing</a> issues as well unless it’s not misused.</p>

<h3 id="data">Data</h3>

<ul>
  <li>To start with we require only few sets of images for transfer learning</li>
  <li>Verifying the result is a bit challenging but we could leverage the labelled data.</li>
</ul>

<h3 id="methods-to-experiment">Methods to experiment</h3>

<ul>
  <li>For transfer learning we an use different sets of custom images for different classes. In my case I have use images of Temple and few images for background. The new class we train will be on top of the previous classes. At the end we give the instruction in the form of speech or text (that covers all the classes and identifiers) to generate the synthesized images.</li>
</ul>

<p>Example images generated from stable diffusion model:</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/multi-class-image-generation-stable-diffusion-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/multi-class-image-generation-stable-diffusion-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/multi-class-image-generation-stable-diffusion-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/multi-class-image-generation-stable-diffusion.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h3 id="jupyter-notebook---tryout-out">Jupyter Notebook - Tryout out</h3>

<p>Download this Jupyter Notebook <a href="assets/ipynb/multi_class_image_generation_using_dreambooth_stable_diffusion.ipynb"> multi_class_image_generation_using_dreambooth_stable_diffusion.ipynb</a> for training your own set of images &amp; for the inference.</p>

<p><strong>Steps</strong></p>

<ul>
  <li>Train &amp; Save Model
    <hr />

    <p><img width="944" alt="image" src="https://github.com/vinapp/vinapp.github.io/assets/8567548/21e11ac2-5363-42c9-97d8-744422234fe0" /></p>
    <hr />

    <p><img width="935" alt="image" src="https://github.com/vinapp/vinapp.github.io/assets/8567548/a8e77ec7-bbc9-42c8-930c-14047492ea7e" /></p>
    <hr />

    <p><img width="930" alt="image" src="https://github.com/vinapp/vinapp.github.io/assets/8567548/4ba43e09-03df-4e4c-974f-636e4b016a47" /></p>
  </li>
  <li>Image Generation
    <hr />

    <p><img width="940" alt="image" src="https://github.com/vinapp/vinapp.github.io/assets/8567548/1f88f34e-23e2-4c39-84a1-9a4482a0f6ca" /></p>
    <hr />

    <p><img width="783" alt="image" src="https://github.com/vinapp/vinapp.github.io/assets/8567548/7860471d-1e35-445a-b08b-b8d8200ed143" /></p>
    <hr />
  </li>
</ul>

<h3 id="result-evaluation">Result Evaluation</h3>

<ul>
  <li>Fidelity: the quality of the generated samples. Measures how realistic the images are. You can think of it as how different each fake sample is from its nearest real sample.</li>
  <li>Diversity: the variety of the generated samples. Measures how well the generated images cover the whole diversity or variety of the real distribution.</li>
</ul>

<p>We can use the labelled trained images to match against the generated images and for this we need more images to train the labelled data.</p>

<h3 id="references">References</h3>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Stable\_Diffusion">Stable Diffusion</a></li>
  <li><a href="https://arxiv.org/pdf/2208.12242.pdf">Dream booth Diffusion</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Diffusion_model">Diffusion Model</a></li>
  <li>
    <p><a href="https://huggingface.co/CompVis/stable-diffusion">Stable Diffusion Model</a></p>

    <p><strong>Related Work/Articles</strong></p>

    <ul>
      <li><a href="https://techpp.com/2022/10/10/how-to-train-stable-diffusion-ai-dreambooth/">Train Stable Diffusion Model</a></li>
      <li><a href="https://bytexd.com/how-to-use-dreambooth-to-fine-tune-stable-diffusion-colab/">Fine Tune Stable Diffusion Model</a></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="ai" /><category term="ml" /><summary type="html"><![CDATA[Voice based Multi class image generation using Stable Diffusion]]></summary></entry></feed>